{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-KqvIyxOcYq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSHO4WgNO59z",
        "outputId": "f1aee6c1-7238-4e77-afe4-81d8b539967c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-27e142b34f24>:1: DtypeWarning: Columns (1,2,8,9,14,19,20,172,173) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df=pd.read_csv('ibtracs.ALL.list.v04r01 (1).csv')\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('ibtracs.ALL.list.v04r01 (1).csv', low_memory=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0HO_Y5Xx7VH"
      },
      "outputs": [],
      "source": [
        "def preprocess_cyclone_data(df, task='path'):\n",
        "    \"\"\"\n",
        "    Preprocesses cyclone data for LSTM and ML models.\n",
        "\n",
        "    Parameters:\n",
        "        df (DataFrame): Raw cyclone dataset.\n",
        "        task (str): 'path' for trajectory prediction (LSTM), 'speed_dir' for speed/direction prediction (ML).\n",
        "\n",
        "    Returns:\n",
        "        Processed DataFrame (X, y) and scalers (if LSTM).\n",
        "    \"\"\"\n",
        "    df = df.copy()  # Avoid modifying original data\n",
        "\n",
        "    # Converting time column to datetime\n",
        "    df['ISO_TIME'] = pd.to_datetime(df['ISO_TIME'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
        "\n",
        "    # Setting index for resampling\n",
        "    df.set_index('ISO_TIME', inplace=True)\n",
        "\n",
        "    # Dealing with duplicate timestamps (keep first occurrence)\n",
        "    df = df[~df.index.duplicated(keep='first')]\n",
        "\n",
        "    # Converting to numeric columns before resampling\n",
        "    for col in ['STORM_SPEED', 'STORM_DIR', 'LAT', 'LON']:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Converting objects to numeric before interpolating\n",
        "    df = df.infer_objects(copy=False)\n",
        "\n",
        "    # data will be resampled every 6 hours and interpolate missing values\n",
        "    df = df.resample('6h').interpolate()\n",
        "\n",
        "    # Reset index after resampling\n",
        "    df.reset_index(inplace=True)\n",
        "\n",
        "    # Circular encoding for wind direction\n",
        "    df['dir_sin'] = np.sin(np.deg2rad(df['STORM_DIR']))\n",
        "    df['dir_cos'] = np.cos(np.deg2rad(df['STORM_DIR']))\n",
        "\n",
        "    # Interaction terms using available parameters\n",
        "    df['lat_lon_interaction'] = df['LAT'] * df['LON']\n",
        "    df['speed_lat_interaction'] = df['STORM_SPEED'] * df['LAT']\n",
        "    df['speed_lon_interaction'] = df['STORM_SPEED'] * df['LON']\n",
        "\n",
        "    # some lag features\n",
        "    df['STORM_SPEED_LAG1'] = df['STORM_SPEED'].shift(1)\n",
        "    df['LAT_LAG'] = df['LAT'].shift(1)\n",
        "    df['LON_LAG'] = df['LON'].shift(1)\n",
        "\n",
        "    # creating moving Averages\n",
        "    df['LAT_MA3'] = df['LAT'].rolling(window=3).mean()\n",
        "    df['LON_MA3'] = df['LON'].rolling(window=3).mean()\n",
        "    df['SPEED_MA3'] = df['STORM_SPEED'].rolling(window=3).mean()\n",
        "\n",
        "    # Deriving dist change and speed change\n",
        "    df['DIST_CHANGE'] = np.sqrt((df['LAT'] - df['LAT_LAG'])**2 + (df['LON'] - df['LON_LAG'])**2)\n",
        "    df['SPEED_CHANGE'] = df['STORM_SPEED'].diff()\n",
        "\n",
        "    # Standard deviation over 3 periods\n",
        "    df['SPEED_STD3'] = df['STORM_SPEED'].rolling(window=3).std()\n",
        "\n",
        "    # mapping time-based features\n",
        "    df['YEAR'] = df['ISO_TIME'].dt.year\n",
        "    df['MONTH'] = df['ISO_TIME'].dt.month\n",
        "    df['DAY'] = df['ISO_TIME'].dt.day\n",
        "    df['HOUR'] = df['ISO_TIME'].dt.hour\n",
        "\n",
        "    # Filling missing values with forward fill\n",
        "    df.ffill(inplace=True)\n",
        "\n",
        "    # Defining feature sets based on task\n",
        "    if task == 'path':\n",
        "        features = ['LAT', 'LON', 'STORM_SPEED', 'HOUR', 'MONTH',\n",
        "                    'lat_lon_interaction', 'speed_lat_interaction', 'speed_lon_interaction',\n",
        "                    'dir_sin', 'dir_cos']\n",
        "        target_cols = ['LAT', 'LON']  # Predicting future position\n",
        "    elif task == 'speed_dir':\n",
        "        features = ['LAT', 'LON', 'HOUR', 'MONTH', 'dir_sin', 'dir_cos',\n",
        "                    'STORM_SPEED_LAG1', 'LAT_LAG', 'LON_LAG',\n",
        "                    'LAT_MA3', 'LON_MA3', 'SPEED_MA3', 'SPEED_STD3']\n",
        "        target_cols = ['STORM_SPEED', 'STORM_DIR']  # Predicting speed and direction\n",
        "    else:\n",
        "        raise ValueError(\"Invalid task type! Choose 'path' or 'speed_dir'.\")\n",
        "\n",
        "    # Dropping NaN values after feature creation\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Extracting features and target\n",
        "    X = df[features].values\n",
        "    y = df[target_cols].values\n",
        "\n",
        "    # Scaling (only for LSTM )\n",
        "    if task == 'path':\n",
        "        scaler_X = StandardScaler()\n",
        "        scaler_y = StandardScaler()\n",
        "        X = scaler_X.fit_transform(X)\n",
        "        y = scaler_y.fit_transform(y)\n",
        "\n",
        "        # got to reshape for LSTM [samples, timesteps, features]\n",
        "        X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
        "\n",
        "        return X, y, scaler_X, scaler_y  # Returning scalers for inverse transformation\n",
        "\n",
        "    return X, y  # our ML models donâ€™t need reshaping or scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgyKzgLSzxW_",
        "outputId": "9a9c044b-e6d2-4b06-c82a-027d3814026c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-5f94d251fc41>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[col] = pd.to_numeric(df[col], errors='coerce')\n",
            "<ipython-input-12-5f94d251fc41>:35: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
            "  df = df.resample('6h').interpolate()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âš¡ **Speed Model Performance**\n",
            "MAE: 0.9540082789638384\n",
            "RÂ² Score: 0.93415075550222\n",
            "\n",
            "ðŸ§­ **Direction Model Performance**\n",
            "MAE: 0.10304005607331024\n",
            "RÂ² Score: 0.9999960459225579\n"
          ]
        }
      ],
      "source": [
        "# Preprocess the data using previously built function\n",
        "X, y = preprocess_cyclone_data(df, task='speed_dir')  # X = features, y = [STORM_SPEED, STORM_DIR]\n",
        "\n",
        "# Splitting into training & test sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Extracting target variables\n",
        "y_train_speed, y_train_dir = y_train[:, 0], y_train[:, 1]  # Speed & Direction separately\n",
        "y_test_speed, y_test_dir = y_test[:, 0], y_test[:, 1]\n",
        "\n",
        "# making XGBoost models\n",
        "speed_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "dir_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Training the models\n",
        "speed_model.fit(X_train, y_train_speed)\n",
        "dir_model.fit(X_train, y_train_dir)\n",
        "\n",
        "# Predicting on test data\n",
        "y_pred_speed = speed_model.predict(X_test)\n",
        "y_pred_dir = dir_model.predict(X_test)\n",
        "\n",
        "# Lets evaluate performance\n",
        "print(\"Speed Model Performance\")\n",
        "print(\"MAE:\", mean_absolute_error(y_test_speed, y_pred_speed))\n",
        "print(\"RÂ² Score:\", r2_score(y_test_speed, y_pred_speed))\n",
        "\n",
        "print(\"\\n Direction Model Performance**\")\n",
        "print(\"MAE:\", mean_absolute_error(y_test_dir, y_pred_dir))\n",
        "print(\"RÂ² Score:\", r2_score(y_test_dir, y_pred_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nY72sRhxP7WN",
        "outputId": "f469fd97-3f08-43ac-a565-d2ebf6744a89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Models saved successfully!\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "\n",
        "joblib.dump(speed_model, \"speed_model.pkl\")  # Saving speed prediction model\n",
        "joblib.dump(dir_model, \"dir_model.pkl\")  # Saving direction prediction model\n",
        "\n",
        "print(\"âœ… Models saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "480Ky1wK5r4i",
        "outputId": "6f53c709-9cef-4887-92b8-b172e009fb9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2527/2527\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5ms/step - loss: 0.0727 - mae: 0.1120 - val_loss: 1.0720e-04 - val_mae: 0.0076\n",
            "Epoch 2/50\n",
            "\u001b[1m2527/2527\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 8.4055e-05 - mae: 0.0066 - val_loss: 6.6684e-05 - val_mae: 0.0065\n",
            "Epoch 3/50\n",
            "\u001b[1m2527/2527\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - loss: 5.0780e-05 - mae: 0.0052 - val_loss: 8.8522e-05 - val_mae: 0.0071\n",
            "Epoch 4/50\n",
            "\u001b[1m2527/2527\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4ms/step - loss: 4.1799e-05 - mae: 0.0046 - val_loss: 3.0547e-05 - val_mae: 0.0039\n",
            "Epoch 5/50\n",
            "\u001b[1m2527/2527\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 3.0355e-05 - mae: 0.0040 - val_loss: 2.0953e-05 - val_mae: 0.0038\n",
            "Epoch 6/50\n",
            "\u001b[1m2527/2527\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - loss: 2.2312e-05 - mae: 0.0035 - val_loss: 8.9374e-06 - val_mae: 0.0023\n",
            "Epoch 7/50\n",
            "\u001b[1m2527/2527\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 2.0813e-05 - mae: 0.0032 - val_loss: 2.0565e-05 - val_mae: 0.0034\n",
            "Epoch 8/50\n",
            "\u001b[1m2527/2527\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 2.0385e-05 - mae: 0.0032 - val_loss: 2.5539e-05 - val_mae: 0.0040\n",
            "Epoch 9/50\n",
            "\u001b[1m2527/2527\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 2.0097e-05 - mae: 0.0032 - val_loss: 2.6130e-05 - val_mae: 0.0040\n",
            "Epoch 10/50\n",
            "\u001b[1m2527/2527\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 2.2340e-05 - mae: 0.0033 - val_loss: 1.3985e-05 - val_mae: 0.0028\n",
            "Epoch 11/50\n",
            "\u001b[1m2527/2527\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 1.9288e-05 - mae: 0.0031 - val_loss: 1.6827e-05 - val_mae: 0.0031\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m632/632\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
            "Actual: [17.6 87.2], Predicted: [17.537634 86.9107  ]\n",
            "Actual: [18.2 84.6], Predicted: [18.213432 84.32055 ]\n",
            "Actual: [ 11.5 111.7], Predicted: [ 11.43135 111.57744]\n",
            "Actual: [ 25.  -60.4], Predicted: [ 24.967434 -60.16016 ]\n",
            "Actual: [-14.8  76. ], Predicted: [-14.761486  75.89571 ]\n"
          ]
        }
      ],
      "source": [
        "def preprocess_cyclone_data_lstm(df, scaler_X=None, scaler_y=None, training=True):\n",
        "    \"\"\"Preprocess cyclone data for LSTM training and inference.\"\"\"\n",
        "\n",
        "    df = df.copy()  # Avoid modifying the original DataFrame\n",
        "    df['ISO_TIME'] = pd.to_datetime(df['ISO_TIME'], errors='coerce')\n",
        "\n",
        "    # Extract time-based features\n",
        "    df['hour'] = df['ISO_TIME'].dt.hour.fillna(0).astype(int)\n",
        "    df['month'] = df['ISO_TIME'].dt.month.fillna(0).astype(int)\n",
        "\n",
        "    # Convert numeric columns to float\n",
        "    numeric_cols = ['LAT', 'LON', 'STORM_SPEED', 'STORM_DIR']\n",
        "    for col in numeric_cols:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Filling missing values with median instead of dropping data\n",
        "    df.fillna(df.median(numeric_only=True), inplace=True)\n",
        "\n",
        "    # Convert storm direction to sine & cosine\n",
        "    df['dir_sin'] = np.sin(np.deg2rad(df['STORM_DIR']))\n",
        "    df['dir_cos'] = np.cos(np.deg2rad(df['STORM_DIR']))\n",
        "\n",
        "    # Interaction terms\n",
        "    df['lat_lon_interaction'] = df['LAT'] * df['LON']\n",
        "    df['speed_lat_interaction'] = df['STORM_SPEED'] * df['LAT']\n",
        "    df['speed_lon_interaction'] = df['STORM_SPEED'] * df['LON']\n",
        "\n",
        "    # feature columns\n",
        "    features = [\n",
        "        'LAT', 'LON', 'STORM_SPEED', 'hour', 'month',\n",
        "        'lat_lon_interaction', 'speed_lat_interaction', 'speed_lon_interaction',\n",
        "        'dir_sin', 'dir_cos'\n",
        "    ]\n",
        "    target_cols = ['LAT', 'LON']\n",
        "\n",
        "    # Extract feature values\n",
        "    X = df[features].values\n",
        "    y = df[target_cols].values if training else None\n",
        "\n",
        "    # Load or fit scalers\n",
        "    if training:\n",
        "        scaler_X = StandardScaler()\n",
        "        scaler_y = StandardScaler()\n",
        "        X_scaled = scaler_X.fit_transform(X)\n",
        "        y_scaled = scaler_y.fit_transform(y)\n",
        "\n",
        "        # Save scalers for later inference\n",
        "        joblib.dump(scaler_X, \"scaler_X.pkl\")\n",
        "        joblib.dump(scaler_y, \"scaler_y.pkl\")\n",
        "\n",
        "    else:\n",
        "        scaler_X = joblib.load(\"scaler_X.pkl\")\n",
        "        X_scaled = scaler_X.transform(X)\n",
        "        y_scaled = None\n",
        "\n",
        "    # Reshape for LSTM input (samples, timesteps=1, features)\n",
        "    X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
        "\n",
        "    return X_scaled, y_scaled, scaler_X, scaler_y\n",
        "\n",
        "# Load dataset (Ensure `df` is available)\n",
        "X_scaled, y_scaled, scaler_X, scaler_y = preprocess_cyclone_data_lstm(df)\n",
        "\n",
        "# Split data for training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build LSTM Model\n",
        "model = Sequential([\n",
        "    LSTM(64, return_sequences=False, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(2)  # Predicting (LAT, LON)\n",
        "])\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Define Early Stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train Model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50, batch_size=32,\n",
        "    validation_data=(X_test, y_test),\n",
        "    verbose=1, callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Save trained model for later inference\n",
        "model.save(\"cyclone_lstm_model.h5\")\n",
        "\n",
        "# Predict\n",
        "y_pred_scaled = model.predict(X_test)\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
        "\n",
        "# Display Sample Predictions\n",
        "y_test_actual = scaler_y.inverse_transform(y_test)\n",
        "for i in range(5):\n",
        "    print(f\"Actual: {y_test_actual[i]}, Predicted: {y_pred[i]}\")\n",
        "\n",
        "# Evaluate on test set (original scale)\n",
        "lat_mae = mean_absolute_error(y_test_actual[:, 0], y_pred[:, 0])\n",
        "lon_mae = mean_absolute_error(y_test_actual[:, 1], y_pred[:, 1])\n",
        "lat_rmse = np.sqrt(mean_squared_error(y_test_actual[:, 0], y_pred[:, 0]))\n",
        "lon_rmse = np.sqrt(mean_squared_error(y_test_actual[:, 1], y_pred[:, 1]))\n",
        "lat_r2 = r2_score(y_test_actual[:, 0], y_pred[:, 0])\n",
        "lon_r2 = r2_score(y_test_actual[:, 1], y_pred[:, 1])\n",
        "overall_r2 = r2_score(y_test_actual, y_pred, multioutput='variance_weighted')\n",
        "overall_rmse = np.sqrt(mean_squared_error(y_test_actual, y_pred))\n",
        "\n",
        "print(\"\\nLSTM Path Model Performance\")\n",
        "print(f\"LAT -> MAE: {lat_mae:.4f}, RMSE: {lat_rmse:.4f}, RÂ²: {lat_r2:.4f}\")\n",
        "print(f\"LON -> MAE: {lon_mae:.4f}, RMSE: {lon_rmse:.4f}, RÂ²: {lon_r2:.4f}\")\n",
        "print(f\"Overall -> RMSE: {overall_rmse:.4f}, RÂ²: {overall_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "iAXjN1bo5sUw",
        "outputId": "52f35477-0108-4c4b-8129-70ceb57d4774"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'load_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-85a4754197b7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load trained model and scalers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cyclone_lstm_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mscaler_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scaler_X.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mscaler_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scaler_y.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
          ]
        }
      ],
      "source": [
        "# Load trained model and scalers\n",
        "model = load_model(\"cyclone_lstm_model.h5\")\n",
        "scaler_X = joblib.load(\"scaler_X.pkl\")\n",
        "scaler_y = joblib.load(\"scaler_y.pkl\")\n",
        "\n",
        "# Load new cyclone data (`new_df` should be a Pandas DataFrame)\n",
        "X_new, _, _, _ = preprocess_cyclone_data_lstm(new_df, training=False)\n",
        "\n",
        "# Predict\n",
        "y_pred_scaled = model.predict(X_new)\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
        "\n",
        "# Print predicted locations\n",
        "print(\"Predicted cyclone locations (LAT, LON):\", y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueR9WYYsgoDy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
